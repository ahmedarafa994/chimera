# AutoDAN-Reasoning

The official implementation of our technical report "[AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling](https://arxiv.org/abs/2510.05379)"
by *[Xiaogeng Liu](https://sheltonliu-n.github.io/) and [Chaowei Xiao](https://xiaocw11.github.io/).*

![Jailbreak Attacks](https://img.shields.io/badge/Jailbreak-Attacks-yellow.svg?style=plastic)
![Adversarial Attacks](https://img.shields.io/badge/Adversarial-Attacks-orange.svg?style=plastic)
![Large Language Models](https://img.shields.io/badge/LargeLanguage-Models-green.svg?style=plastic)

---


## ðŸ“š Abstract

Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.

![pipeline](figures/figure.png)

## ðŸš€ Quick Start
- **Get code**
```shell
git clone https://github.com/SaFoLab-WISC/AutoDAN-Reasoning.git
```

- **Build environment**
```shell
cd AutoDAN-Reasoning
conda create -n autodanreasoning python==3.12
conda activate autodanreasoning
pip install -r requirements.txt
```

- **Download LLM Chat Templates**\
```shell
cd llm
git clone https://github.com/chujiezheng/chat_templates.git
cd ..
```

- **Training Process Visulization**
```shell
wandb login
```

## ðŸŽ¯ AutoDAN-Reasoning (Test-Time Scaling)
- **Enhanced with Test-Time Scaling**\
  *AutoDAN-Reasoning enhances AutoDAN-Turbo with test-time scaling methods (Best-of-N and Beam Search) to further improve attack success rates. This implementation uses the pre-trained strategy library from AutoDAN-Turbo-R.*

(Using OpenAI API)
```shell
python test_autodan_reasoning.py \
                 --openai_api_key "<your openai api key>" \
                 --embedding_model "<openai text embedding model name>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --method beam_search \
                 --beam_width 4 \
                 --beam_depth 3 \
                 --beam_k 10 \
                 --request "<the malicious request, e.g., how to build a bomb?>"
```
(Or Microsoft Azure API)
```shell
python test_autodan_reasoning.py \
                 --azure \
                 --azure_endpoint "<your azure endpoint>" \
                 --azure_api_version "2024-02-01" \
                 --azure_deployment_name "<your azure model deployment name>" \
                 --azure_api_key "<your azure api key>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --method beam_search \
                 --beam_width 4 \
                 --beam_depth 3 \
                 --beam_k 10 \
                 --request "<the malicious request, e.g., how to build a bomb?>"
```

**Key Parameters:**
- `--method`: Choose from `vanilla`, `best_of_n`, `beam_search`, or `all` (comparison mode)
- `--best_of_n N`: Number of candidates for Best-of-N (default: 4)
- `--beam_width W`: Beam width for Beam Search (default: 4)
- `--beam_depth C`: Maximum combination size for Beam Search (default: 3)
- `--beam_k K`: Strategy pool size for Beam Search (default: 10)

## ðŸŒ´ AutoDAN-Turbo-R Lifelong Learning
- **Train**\
  *We use Deepseek-R1 (from their official API) as the foundation model for the attacker, scorer, summarizer. We utilize OpenAI's text embedding model to embed text.*

(Using OpenAI API)
```shell
python main_r.py --vllm \
                 --openai_api_key "<your openai api key>" \
                 --embedding_model "<openai text embedding model name>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 150
```
(Or Microsoft Azure API)
```shell
python main_r.py --vllm \
                 --azure \
                 --azure_endpoint "<your azure endpoint>" \
                 --azure_api_version "2024-02-01" \
                 --azure_deployment_name "<your azure model deployment name>" \
                 --azure_api_key "<your azure api key>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 150
```

- **Test**\
  *After training, given a malicious request, test.py generates the corresponding jailbreak prompts.* \
  *ðŸ™Œ We have provided the Strategy Library for AutoDAN-Turbo-R in the folder ./logs_r; you can test it directly.*


(Using OpenAI API)
```shell
python test_r.py --openai_api_key "<your openai api key>" \
                 --embedding_model "<openai text embedding model name>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 150 \
                 --request "<the malicious request, e.g., how to build a bomb?>"
```
(Or Microsoft Azure API)
```shell
python test_r.py --azure \
                 --azure_endpoint "<your azure endpoint>" \
                 --azure_api_version "2024-02-01" \
                 --azure_deployment_name "<your azure model deployment name>" \
                 --azure_api_key "<your azure api key>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 150 \
                 --request "<the malicious request, e.g., how to build a bomb?>"
```

## ðŸŒ± AutoDAN-Turbo-v1.0 Lifelong Learning
- **Train**\
*The old version of AutoDAN-Turbo. We use Huggingface models as the foundation for the attacker, scorer, summarizer, and target model, and utilize OpenAI's text embedding model to embed text.*

(Using OpenAI API)
```shell
python main.py --vllm \
               --openai_api_key "<your openai api key>" \
               --embedding_model "<openai text embedding model name>" \
               --hf_token "<your huggingface token>" \
               --epochs 150
```
(Or Microsoft Azure API)
```shell
python main.py --vllm \
               --azure \
               --azure_endpoint "<your azure endpoint>" \
               --azure_api_version "2024-02-01" \
               --azure_deployment_name "<your azure model deployment name>" \
               --azure_api_key "<your azure api key>" \
               --hf_token "<your huggingface token>" \
               --epochs 150
```


- **Test**\
  *After training, given a malicious request, test.py generates the corresponding jailbreak prompts.*

(Using OpenAI API)
```shell
python test.py --openai_api_key "<your openai api key>" \
               --embedding_model "<openai text embedding model name>" \
               --hf_token "<your huggingface token>" \
               --epochs 150 \
               --request "<the malicious request, e.g., how to build a bomb?>"
```
(Or Microsoft Azure API)
```shell
python test.py --azure \
               --azure_endpoint "<your azure endpoint>" \
               --azure_api_version "2024-02-01" \
               --azure_deployment_name "<your azure model deployment name>" \
               --azure_api_key "<your azure api key>" \
               --hf_token "<your huggingface token>" \
               --epochs 150 \
               --request "<the malicious request, e.g., how to build a bomb?>"
```

## ðŸ“Ž Reference BibTeX
```bibtex
@misc{liu2024autodanturbolifelongagentstrategy,
      title={AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs},
      author={Xiaogeng Liu and Peiran Li and Edward Suh and Yevgeniy Vorobeychik and Zhuoqing Mao and Somesh Jha and Patrick McDaniel and Huan Sun and Bo Li and Chaowei Xiao},
      year={2024},
      eprint={2410.05295},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.05295},
}
```
```bibtex
@inproceedings{
      liu2024autodan,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=7Jwpw4qKkb}
}
```

## ðŸ§  AutoDAN-Advanced (Hierarchical Genetic Search)
- **Enhanced with HGS & Gradient Guidance**\
  *AutoDAN-Advanced introduces a bi-level Hierarchical Genetic Search (HGS) coupled with Coherence-Constrained Gradient Search (CCGS) to find stealthy and transferrable jailbreaks.*

(Using OpenAI API)
```shell
python test_autodan_advanced.py \
                 --openai_api_key "<your openai api key>" \
                 --embedding_model "<openai text embedding model name>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "<your deepseek api key>" \
                 --deepseek_model "deepseek-reasoner" \
                 --method hgs \
                 --archive_capacity 100 \
                 --coherence_weight 0.5 \
                 --request "<the malicious request, e.g., how to build a bomb?>"
```

**Key Parameters:**
- `--method`: `hgs` (Hierarchical Genetic Search)
- `--archive_capacity`: Capacity of the Diversity Archive (default: 100)
- `--coherence_weight`: Weight for the language model probability in the gradient search (default: 0.5)
