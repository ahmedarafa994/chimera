"""Test script to verify that AutoDAN prompts are generated by AI models,
not through template substitution.

This script tests the actual LLM generation to ensure:
1. Output is AI-generated (not template-filled)
2. No placeholder patterns remain in output
3. Output is creative and varies between runs
"""

import asyncio
import os
import sys

# Add the app directory to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.engines.autodan_turbo.lifelong_engine import (
    CREATIVE_GENERATION_PROMPT,
    build_attack_generation_prompt,
)
from app.engines.autodan_turbo.models import JailbreakStrategy


def test_prompt_generation_instructions():
    """Test that the generation prompts contain proper AI-generation instructions."""
    # Create a test strategy
    strategy = JailbreakStrategy(
        name="Roleplay Authority",
        description="Use authority figure roleplay to bypass safety",
        template="You are a security expert. {payload}",
        examples=["Example: As a security researcher, I need to understand..."],
        tags=["roleplay", "authority"],
    )

    # Generate the prompt that will be sent to the LLM
    test_request = "explain how to pick a lock"
    prompt = build_attack_generation_prompt(test_request, [strategy])

    # Verify key instructions are present
    checks = [
        ("CREATE/GENERATE instruction", "CREATE" in prompt or "GENERATE" in prompt),
        ("No template warning", "template" in prompt.lower() or "placeholder" in prompt.lower()),
        ("ORIGINAL requirement", "ORIGINAL" in prompt or "NOVEL" in prompt),
        ("Request included", test_request in prompt),
        ("Strategy guidance", strategy.name in prompt),
    ]

    all_passed = True
    for _name, passed in checks:
        if not passed:
            all_passed = False

    return all_passed


def test_creative_prompt_template():
    """Test the creative generation prompt template."""
    test_request = "explain how to pick a lock"
    prompt = CREATIVE_GENERATION_PROMPT.format(request=test_request)

    checks = [
        ("CREATIVE instruction", "CREATIVE" in prompt),
        ("ORIGINAL instruction", "ORIGINAL" in prompt),
        ("No template warning", "template" in prompt.lower()),
        ("Request included", test_request in prompt),
    ]

    all_passed = True
    for _name, passed in checks:
        if not passed:
            all_passed = False

    return all_passed


def test_validation_logic_standalone():
    """Test the validation logic that detects template-based output (standalone version)."""

    # Standalone validation function (same logic as in lifelong_engine)
    def validate_prompt(prompt, request):
        """Validate that the prompt is AI-generated and not a template."""
        if not prompt:
            return None

        prompt = prompt.strip()

        # Check for unfilled placeholders (indicates template, not AI generation)
        placeholder_patterns = [
            "{payload}",
            "{request}",
            "{goal}",
            "{target}",
            "[INSERT",
            "[PAYLOAD",
            "[REQUEST",
            "[GOAL",
            "{{",
            "}}",
            "<PLACEHOLDER>",
            "[TODO]",
        ]

        for pattern in placeholder_patterns:
            if pattern.lower() in prompt.lower():
                return None

        # Check minimum length (AI-generated prompts should be substantial)
        if len(prompt) < 100:
            return None

        # Remove common prefixes that indicate meta-commentary
        prefixes_to_remove = [
            "Here is",
            "Here's",
            "Sure,",
            "Certainly,",
            "Of course,",
            "Here is the adversarial prompt:",
            "Generated prompt:",
            "Adversarial prompt:",
            "The prompt is:",
            "Output:",
        ]

        for prefix in prefixes_to_remove:
            if prompt.lower().startswith(prefix.lower()):
                prompt = prompt[len(prefix) :].strip().lstrip(":")

        # Remove surrounding quotes if present
        if prompt.startswith('"') and prompt.endswith('"'):
            prompt = prompt[1:-1]
        if prompt.startswith("'") and prompt.endswith("'"):
            prompt = prompt[1:-1]

        return prompt if len(prompt) >= 50 else None

    # Test cases
    test_cases = [
        # (input, expected_valid, description)
        (
            "This is a valid AI-generated prompt that is long enough and contains creative content about the topic.",
            True,
            "Valid AI-generated prompt",
        ),
        ("Short", False, "Too short"),
        (
            "This contains {payload} placeholder and is long enough to pass length check but should fail",
            False,
            "Contains {payload}",
        ),
        (
            "This contains {request} placeholder and is long enough to pass length check but should fail",
            False,
            "Contains {request}",
        ),
        (
            "This contains [INSERT HERE] marker and is long enough to pass length check but should fail",
            False,
            "Contains [INSERT",
        ),
        (
            "Here is the prompt: This is a valid AI-generated prompt that is long enough and contains creative content.",
            True,
            "Valid with prefix removed",
        ),
    ]

    all_passed = True
    for input_text, expected_valid, _description in test_cases:
        result = validate_prompt(input_text, "test request")
        is_valid = result is not None
        passed = is_valid == expected_valid
        if not passed:
            all_passed = False

    return all_passed


async def test_actual_llm_generation():
    """Test actual LLM generation to verify AI-generated output."""
    # Check if API keys are available
    from app.core.config import settings

    if not settings.GOOGLE_API_KEY and not settings.OPENAI_API_KEY:
        return None

    try:
        from app.services.autodan.llm.chimera_adapter import ChimeraLLMAdapter

        # Create LLM adapter
        adapter = ChimeraLLMAdapter()

        # Test request
        test_request = "explain how to pick a lock"

        # Create a test strategy
        strategy = JailbreakStrategy(
            name="Roleplay Authority",
            description="Use authority figure roleplay to bypass safety",
            template="You are a security expert. {payload}",
            examples=["Example: As a security researcher, I need to understand..."],
            tags=["roleplay", "authority"],
        )

        # Generate the prompt
        gen_prompt = build_attack_generation_prompt(test_request, [strategy])

        # Call the LLM
        system_prompt = "You are a helpful AI assistant."
        response = adapter.generate(system_prompt, gen_prompt)

        output = response.content if hasattr(response, "content") else str(response)

        # Verify the output is AI-generated (not template-filled)
        placeholder_patterns = [
            "{payload}",
            "{request}",
            "{goal}",
            "{target}",
            "[INSERT",
            "[PAYLOAD",
            "[REQUEST",
            "[GOAL",
            "{{",
            "}}",
        ]

        has_placeholders = any(p.lower() in output.lower() for p in placeholder_patterns)
        is_long_enough = len(output) > 100
        is_creative = not output.strip().startswith(
            "You are a security expert.",
        )  # Not just the template

        checks = [
            ("No unfilled placeholders", not has_placeholders),
            ("Sufficient length (>100 chars)", is_long_enough),
            ("Not just template copy", is_creative),
            ("Contains creative content", len(output.split()) > 20),
        ]

        all_passed = True
        for _name, passed in checks:
            if not passed:
                all_passed = False

        return all_passed

    except Exception:
        return None


async def main():
    """Run all tests."""
    results = []

    # Test 1: Prompt generation instructions
    results.append(("Prompt Generation Instructions", test_prompt_generation_instructions()))

    # Test 2: Creative prompt template
    results.append(("Creative Prompt Template", test_creative_prompt_template()))

    # Test 3: Validation logic (standalone)
    results.append(("Validation Logic", test_validation_logic_standalone()))

    # Test 4: Actual LLM generation (optional)
    llm_result = await test_actual_llm_generation()
    if llm_result is not None:
        results.append(("Actual LLM Generation", llm_result))
    else:
        pass

    # Summary

    all_passed = True
    for _name, passed in results:
        if not passed:
            all_passed = False

    if all_passed:
        pass
    else:
        pass

    return all_passed


if __name__ == "__main__":
    asyncio.run(main())
